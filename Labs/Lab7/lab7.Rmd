---
title: 'Lab #7 Report'
author: "Nick Huo"
date: "`r Sys.Date()`"
output: 
    pdf_document:
        fig_width: 5.5
        fig_height: 3.5
        fig_caption: true
papersize: letter
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1

## 1.

```{r}
dat <- read.csv("Lab7.csv")
y <- matrix(dat$Y, ncol=1)
X <- as.matrix(dat[,-1])
```


```{r}
calc_vif <- function(mtx) {
    res <- c()
    for (i in 1:ncol(mtx)) {
        mod <- lm(mtx[,i] ~ mtx[,-i])
        rsq <- summary(mod)$r.squared
        res <- c(res, 1/(1-rsq))
    }
    return(res)
}
calc_vif(X)
```

We see that the VIFs for the columns are all close to 1, 
indicating that the variances are not inflated and there is no multicollinearity.

## 2. 

```{r}
bias_ridge <- function(X, lambda, beta){
  bias <- solve(t(X)%*%X + lambda*diag(1, ncol(t(X)%*%X)) ) %*% (t(X)%*%X) %*% beta - beta
  return(bias)
}

var_ridge <- function(X, lambda, sigma_sq){
  variance <- sigma_sq * solve(t(X)%*%X + lambda*diag(1, ncol(t(X)%*%X)) ) %*% 
      (t(X)%*%X) %*% solve(t(X)%*%X + lambda*diag(1, ncol(t(X)%*%X)) )
  return(variance)
}
```


```{r}
lbd <- 0
beta_ridge <- c(0.398, 0.004, -0.831, 0.292, 0.004)
sigmasq_ridge <- 0.40

bias_ridge(X, lbd, beta_ridge)
diag(var_ridge(X, lbd, sigmasq_ridge))
```

When $\lambda=0$, this is the same as doing Ordinary Least Squares regression.
So we see that the bias is essentially zero, since we know the OLS estimator is unbiased.
And we also see that the variances has values around $0.4$, similar to our $\sigma^2=0.40$.

## 3.

```{r}
lbd <- 2

bias_ridge(X, lbd, beta_ridge)
diag(var_ridge(X, lbd, sigmasq_ridge))
```


## 4.

When $\lambda=0$, the bias is essentially zero.
But when $\lambda=2$, we now see that the bias is not zero any more, despite being a small bias.
And looking at the variances, when $\lambda=2$, the variances are also showing
slightly smaller values, indicating that a smaller variance compared to when $\lambda=0$.
I think this is because that we are seeing the bias-variance trade off, where we achieved 
smaller variance at the cost of the predictor being a bit biased.

## 5.

```{r}
lbds <- seq(0,10, length.out=1000)
biases_ridge <- matrix(NA, 1000, 5)
vars_ridge <- matrix(NA, 1000, 5)
```

## 6.

```{r}
for (i in 1:1000) {
    this_lbd <- lbds[i]
    biases_ridge[i,] <- bias_ridge(X, this_lbd, beta_ridge)
    vars_ridge[i,] <- diag(var_ridge(X, this_lbd, sigmasq_ridge))
}
```

## 7.

```{r}
mse <- matrix(NA, 1000, 5)
for (i in 1:1000) {
    mse[i,] <- vars_ridge[i,] + (biases_ridge[i,])^2
}
```

## 8.

```{r}
plot_est <- function(i) {
    plot(NA, NA, xlab = expression(lambda), ylab ="Values", 
         main=paste(i,"th","estimator"),
         xlim=c(0,10), ylim =c(0,0.008))
    lines(lbds, biases_ridge[,i]^2)
    lines(lbds, vars_ridge[,i], col="blue")
    lines(lbds, mse[,i], col="red")
    legend(7, 0.008, legend=c(expression(Bias^2), "Variance", "MSE"), 
           col=c("black","blue","red"), lty=1)
}

plot_est(1)
plot_est(2)
plot_est(3)
plot_est(4)
plot_est(5)

```

We see that for all the of estimators, as lambda increases, 
$Bias^2$ increases and $Variance$ decreases. Often times, for $\lambda$ values that are large,
the bias will become too big that the decrease in variance is not big enough, and the MSE also 
increases drastically.

## 9.

It seems that $\lambda$ at around 2 works the best, since for most estimators, 
that is where the MSE is the lowest. So we are benefiting from the decrease in variance
but not affected as much by the increase in bias.

## 10.

This procedure is highly subjective and is not precise. So we need some quantitative based method.

## 11.

```{r}
library(glmnet)
ridge_cv <- cv.glmnet(X, y, alpha=0)
plot(ridge_cv, main="Lambda Values for Ridge Regression with Cross-Validation")
ridge_cv$lambda.min
ridge_cv$lambda.1se
```

## 12.

The $\lambda$ values we got from using cross-validation is a lot smaller than 
what I guessed to be the best $\lambda$ value in question 11.

## 13.

I think mainly we are learning to explore the bias-variance trade off.
We see that when there is too much regulation ($\lambda$ is too high), 
we will get too much variance that produces large MSEs so it becomes even worse than using OLS.
So this is why we need to pick $\lambda$ carefully, so that we are able to benefit
from the bias-variance trade off and get lower MSEs with ridge regression.
Just looking at the plots alone, it is unclear what $\lambda$ should we pick exactly.
This is where cross-validation becomes useful and helps us to find the right $\lambda$ to use.




